# -*- coding: utf-8 -*-
"""image_caption

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hk_MqcrH9IQd5-AsgBM8t1-NNOZvK_H9
"""

pip install torch torchvision clip-anytorch pillow

import torch
import clip
from PIL import Image
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"

model, preprocess = clip.load("ViT-B/32", device=device)

def load_captions(path):
    df = pd.read_csv(path)
    return df["Caption"].tolist()

def get_image_embedding(image_path):
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
    with torch.no_grad():
        image_features = model.encode_image(image)
    return image_features / image_features.norm(dim=-1, keepdim=True)

def get_text_embeddings(captions):
    with torch.no_grad():
        text_tokens = clip.tokenize(captions).to(device)
        text_features = model.encode_text(text_tokens)
    return text_features / text_features.norm(dim=-1, keepdim=True)

def recommend_captions(image_path, captions, top_k=5):
    image_feat = get_image_embedding(image_path)
    text_feats = get_text_embeddings(captions)
    similarities = (image_feat @ text_feats.T).squeeze(0)
    top_indices = similarities.topk(top_k).indices.tolist()
    return [captions[i] for i in top_indices]

image_path = "/content/IMG_3429.JPG"
caption_file = "that_girl_captions.csv"

captions = load_captions(caption_file)
top_captions = recommend_captions(image_path, captions, top_k=5)

print("ðŸ“¸ Recommended Captions:")
for cap in top_captions:
    print("â€¢", cap)